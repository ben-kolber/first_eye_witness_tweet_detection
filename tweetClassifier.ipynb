{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import pandas as pd \n",
    "import ahocorasick\n",
    "import regex as re\n",
    "import numpy as np \n",
    "from nltk import word_tokenize, pos_tag_sents, ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentences(sentence):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(text):\n",
    "    for word in text.split():\n",
    "        if word.lower() in contractions:\n",
    "            text = text.replace(word, contractions[word.lower()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(text):\n",
    "    list = []\n",
    "    for x in text:\n",
    "        list.append(' '.join(x))\n",
    "    return list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cell preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(data):\n",
    "\n",
    "    data['text'] = data['text'].apply(lambda x: replace(x))\n",
    "    \n",
    "    data['clean_text'] = np.vectorize(remove_pattern)(data['text'], \"@[\\w]*\")\n",
    "    # remove special characters \n",
    "    data['clean_text'] = data['clean_text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    # remove URL's \n",
    "    data['clean_text'] = np.vectorize(remove_pattern)(data['clean_text'], \"http[\\w]*\")\n",
    "\n",
    "    data['clean_text'] = data['clean_text'].str.lower()\n",
    "\n",
    "    data['stemmed'] = data['clean_text'].apply(stem_sentences)\n",
    "\n",
    "\n",
    "    # generate bigrams and unigrams  \n",
    "    data['unigrams'] = data.apply(lambda row: nltk.word_tokenize(row['stemmed']), axis=1)\n",
    "    \n",
    "    \n",
    "    data['bigrams'] = data['stemmed'].apply(lambda row: list(ngrams(row.split(), 2)))\n",
    "    data['bigrams'] = data['bigrams'].apply(lambda row: join(row))\n",
    "    \n",
    "    data['ngrams'] = data.apply(lambda x: list([x['unigrams'] , x['bigrams']]), axis=1)\n",
    "\n",
    "    #POS tagging \n",
    "    clean_text = data['clean_text'].tolist()\n",
    "    tagged_texts = pos_tag_sents(map(word_tokenize, clean_text))\n",
    "    data['POS_unigrams'] = tagged_texts\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('2013_Queensland_Floods_dev.tsv', sep='\\t')\n",
    "data = data[['text', 'label']]\n",
    "\n",
    "data = preprocess_dataframe(data)\n",
    "\n",
    "#relevant_data = data.loc[data['label'] == 'relevant']\n",
    "#irelevant_data = data.loc[data['label'] == 'not_relevant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aho_corasik(model, lst):\n",
    "    count = 0\n",
    "    for string in lst:\n",
    "        for word in string:\n",
    "            try:\n",
    "                model.get(word)\n",
    "                count +=1\n",
    "            except:\n",
    "                count = count\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features from each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickled trained aho-corasik models \n",
    "pickled_first_person = '/Users/benjaminkolber/Desktop/aho_corasik_trained_models/first_person.pickle'\n",
    "pickled_current ='/Users/benjaminkolber/Desktop/aho_corasik_trained_models/current.pickle'\n",
    "pickled_curse = '/Users/benjaminkolber/Desktop/aho_corasik_trained_models/curse.pickle'\n",
    "pickled_proximity = '/Users/benjaminkolber/Desktop/aho_corasik_trained_models/proximity.pickle'\n",
    "pickled_disasters = '/Users/benjaminkolber/Desktop/aho_corasik_trained_models/disasters.pickle'\n",
    "pickled_caution = '/Users/benjaminkolber/Desktop/aho_corasik_trained_models/caution.pickle'\n",
    "pickled_emotions = '/Users/benjaminkolber/Desktop/aho_corasik_trained_models/emotions.pickle'\n",
    "pickled_perceptual = '/Users/benjaminkolber/Desktop/aho_corasik_trained_models/perceptual.pickle'\n",
    "pickled_dmg = '/Users/benjaminkolber/Desktop/aho_corasik_trained_models/dmg.pickle'\n",
    "pickled_suffering = '/Users/benjaminkolber/Desktop/aho_corasik_trained_models/suffering.pickle'\n",
    "pickled_distress = '/Users/benjaminkolber/Desktop/aho_corasik_trained_models/distress.pickle'\n",
    "pickled_emergency = '/Users/benjaminkolber/Desktop/aho_corasik_trained_models/emergency.pickle'\n",
    "pickled_descriptors = '/Users/benjaminkolber/Desktop/aho_corasik_trained_models/descriptors.pickle'\n",
    "\n",
    "\n",
    "files = [pickled_first_person, pickled_current, pickled_curse, \n",
    "         pickled_proximity, pickled_disasters, pickled_caution, \n",
    "         pickled_emotions, pickled_perceptual, pickled_dmg, \n",
    "         pickled_suffering, pickled_distress, pickled_emergency, \n",
    "         pickled_descriptors]\n",
    "\n",
    "ahocorasik_models = []\n",
    "count = 0\n",
    "for file in files:\n",
    "    fp_file = open(file , 'rb')\n",
    "    #ahocorasik_models.append(\n",
    "    ahocorasik_models.append(pickle.load(fp_file))\n",
    "    time.sleep(0.5)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    feature_matrix = data[['unigrams' , 'bigrams','ngrams' ,'label']]\n",
    "    features = ['is_first_person', 'is_current', 'is_curse', 'is_proximitiy', \n",
    "                'is_disaster', 'is_caution' , 'is_emotion', 'is_preceptual',\n",
    "               'is_dmg' , 'is_suffering', 'is_distress', 'is_emergency', 'is_descriptor']\n",
    "\n",
    "    count = 0\n",
    "    # extract bigram and unigram features\n",
    "    for feature in features:\n",
    "        feature_matrix[feature] = feature_matrix['ngrams'].apply(lambda x: aho_corasik(ahocorasik_models[count],list(x)))\n",
    "        count += 1\n",
    "    return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data \n",
    "feature_matrix = extract_features(data)\n",
    "feature_matrix = feature_matrix.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature_matrix[['is_first_person',\n",
    "       'is_current', 'is_curse', 'is_proximitiy', 'is_disaster', 'is_caution',\n",
    "       'is_emotion', 'is_preceptual', 'is_dmg' , 'is_suffering', 'is_distress', \n",
    "                          'is_emergency', 'is_descriptor']]\n",
    "\n",
    "Y_train = feature_matrix['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, Y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg(X_train, X_test, y_train, y_test):\n",
    "    nums = [10, 50, 100, 500, 1000, 10000]\n",
    "    for num in nums:\n",
    "        logreg = LogisticRegression(max_iter = num)\n",
    "        logreg.fit(X_train, y_train)\n",
    "        Y_pred = logreg.predict(X_test)\n",
    "        acc_log = round(logreg.score(X_train, y_train) * 100, 2)\n",
    "        print('accuracy of logistic regression: {} for max_iter = {}'.format(acc_log, num))\n",
    "        print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc(X_train, X_test, y_train, y_test):\n",
    "    svc = SVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    Y_pred = svc.predict(X_test)\n",
    "    acc_svc = round(svc.score(X_train, y_train) * 100, 2)\n",
    "    print('accuracy of support vector: {}'.format(acc_svc))\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, X_test, y_train, y_test):\n",
    "    knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "    knn.fit(X_train, y_train)\n",
    "    Y_pred = knn.predict(X_test)\n",
    "    acc_knn = round(knn.score(X_train, y_train) * 100, 2)\n",
    "    print('accuracy of k- nearest neighbors: {}'.format(acc_knn))\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(X_train, X_test, y_train, y_test):\n",
    "    gaussian = GaussianNB()\n",
    "    gaussian.fit(X_train, y_train)\n",
    "    Y_pred = gaussian.predict(X_test)\n",
    "    acc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\n",
    "    print('accuracy of guassian: {}'.format(acc_gaussian))\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(X_train, X_test, y_train, y_test):\n",
    "    perceptron = Perceptron()\n",
    "    perceptron.fit(X_train, y_train)\n",
    "    Y_pred = perceptron.predict(X_test)\n",
    "    acc_perceptron = round(perceptron.score(X_train, y_train) * 100, 2)\n",
    "    print('accuracy of perceptron: {}'.format(acc_perceptron))\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, x_test, y_train, y_test):\n",
    "    random_forest = RandomForestClassifier()\n",
    "    random_forest.fit(X_train, y_train)\n",
    "    Y_pred = random_forest.predict(X_test)\n",
    "    acc_rand_forst = round(random_forest.score(X_train, y_train) * 100, 2)\n",
    "    print('accuracy of random forest: {}'.format(acc_rand_forst))\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "quake_test = pd.read_csv('2015_Nepal_Earthquake_test.csv')\n",
    "quake_train = pd.read_csv('2015_Nepal_Earthquake_train.csv')\n",
    "\n",
    "flood_test = pd.read_csv('2013_Queensland_Floods_test.csv')\n",
    "flood_train = pd.read_csv('2013_Queensland_Floods_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing models on different, bigger more mixed scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train + test to two dataframe\n",
    "# preprocess\n",
    "# test on models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = quake_test.append(flood_test , ignore_index=True)\n",
    "data_train = quake_train.append(flood_train , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = preprocess_dataframe(data_test)\n",
    "data_train = preprocess_dataframe(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = extract_features(data_train)\n",
    "test_matrix = extract_features(data_test)\n",
    "\n",
    "train_matrix = train_matrix.sample(frac=1).reset_index(drop=True)\n",
    "test_matrix = test_matrix.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_matrix[['is_first_person',\n",
    "       'is_current', 'is_curse', 'is_proximitiy', 'is_disaster', 'is_caution',\n",
    "       'is_emotion', 'is_preceptual', 'is_dmg' , 'is_suffering', 'is_distress', \n",
    "                          'is_emergency', 'is_descriptor']]\n",
    "\n",
    "X_test = test_matrix[['is_first_person',\n",
    "       'is_current', 'is_curse', 'is_proximitiy', 'is_disaster', 'is_caution',\n",
    "       'is_emotion', 'is_preceptual', 'is_dmg' , 'is_suffering', 'is_distress', \n",
    "                          'is_emergency', 'is_descriptor']]\n",
    "\n",
    "y_train = train_matrix['label']\n",
    "\n",
    "y_test = test_matrix['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of logistic regression: 79.35 for max_iter = 10\n",
      "------------------------------\n",
      "accuracy of logistic regression: 79.35 for max_iter = 50\n",
      "------------------------------\n",
      "accuracy of logistic regression: 79.35 for max_iter = 100\n",
      "------------------------------\n",
      "accuracy of logistic regression: 79.35 for max_iter = 500\n",
      "------------------------------\n",
      "accuracy of logistic regression: 79.35 for max_iter = 1000\n",
      "------------------------------\n",
      "accuracy of logistic regression: 79.35 for max_iter = 10000\n",
      "------------------------------\n",
      "accuracy of k- nearest neighbors: 80.24\n",
      "------------------------------\n",
      "accuracy of perceptron: 78.35\n",
      "------------------------------\n",
      "accuracy of guassian: 76.74\n",
      "------------------------------\n",
      "accuracy of support vector: 79.5\n",
      "------------------------------\n",
      "accuracy of random forest: 81.72\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "log_reg(X_train, X_test, y_train, y_test)\n",
    "knn(X_train, X_test, y_train, y_test)\n",
    "perceptron(X_train, X_test, y_train, y_test)\n",
    "gaussian(X_train, X_test, y_train, y_test)\n",
    "svc(X_train, X_test, y_train, y_test)\n",
    "random_forest(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
